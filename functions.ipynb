{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a3845c-2824-4f60-859a-b0b966b60292",
   "metadata": {},
   "source": [
    "## INITIALIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e5d8923-148f-4768-8e03-e835db930721",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Matcher\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcs\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import constants as cs\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "import PyPDF2 as pdf\n",
    "from docx2pdf import convert\n",
    "import tempfile\n",
    "import streamlit as st\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faf7c42-b8be-49f8-b3d0-2034145fc3ef",
   "metadata": {},
   "source": [
    "## Creating List of Skills for Majors at Norwich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23bd6e35-3dae-4b08-908f-f96183dcadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "major = [\"Architecture\", \"Biology\", \"Business_Administration\", \"Chemistry\", \"Criminal_Justice\", \"Criminology\", \"Communications\", \"English\", \n",
    "          \"Computer_Science\",\"Cybersecurity\", \"Construction_Management\", \"Engineering\", \"Military_Science\", \"Nursing\", \"Psychology\", \"SWAP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57b3271d-d3a6-4776-b82d-6f0961ab0ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Architecture= open(\"Architecture_skills.txt\").read().splitlines()\n",
    "Biology= open(\"Biology_skills.txt\").read().splitlines()\n",
    "Business_Administration= open(\"Bus_Admin_skills.txt\").read().splitlines()\n",
    "Chemistry= open(\"Chemistry_skills.txt\").read().splitlines()\n",
    "Criminal_Justice= open(\"CJandCriminology_skills.txt\").read().splitlines()\n",
    "Criminology= open(\"CJandCriminology_skills.txt\").read().splitlines()\n",
    "Communications= open(\"Communications_skills.txt\").read().splitlines()\n",
    "English= open(\"Communications_skills.txt\").read().splitlines()\n",
    "Computer_Science= open(\"Comp_Sci_skills.txt\").read().splitlines()\n",
    "Construction_Management= open(\"Const_Mana_skills.txt\").read().splitlines()\n",
    "Engineering= open(\"Engineering_skills.txt\").read().splitlines()\n",
    "Military_Science= open(\"Military_skills.txt\").read().splitlines()\n",
    "Nursing= open(\"Nursing_skills.txt\").read().splitlines()\n",
    "Psychology= open(\"Psychology_skills.txt\").read().splitlines()\n",
    "SWAP = open(\"SWAP_skills.txt\").read().splitlines()\n",
    "soft_skills = (\"Soft_skills.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ead2fe6f-2a2a-43f6-8726-1ac81d46c75a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mSWAP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d40ea6d6-b7bd-46d4-811e-68e0cac9a497",
   "metadata": {},
   "source": [
    "## Resume Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "172cb5bc-77f6-4eb6-8ba3-c69358859187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_skills(text):\n",
    "    if text == \"Architecture\":\n",
    "        major_skills = Architecture\n",
    "    elif text == \"Biology\":\n",
    "        major_skills = Biology\n",
    "    elif text == \"Business_Administration\":\n",
    "        major_skills = Business_Administration\n",
    "    elif text == \"Chemistry\":\n",
    "        major_skills = Chemistry\n",
    "    elif text == \"Criminal Justice\":\n",
    "        major_skills = Criminal_Justice\n",
    "    elif text == \"Criminology\":\n",
    "        major_skills = Criminology\n",
    "    elif text == \"Communications\":\n",
    "        major_skills = Communications\n",
    "    elif text == \"English\":\n",
    "        major_skills = English\n",
    "    elif text == \"Computer_Science\":\n",
    "        major_skills = Computer_Science\n",
    "    elif text == \"Construction_Management\":\n",
    "        major_skills = Construction_Management\n",
    "    elif text == \"Engineering\":\n",
    "        major_skills = Engineering\n",
    "    elif text == \"Military_Science\":\n",
    "        major_skills = Military_Science\n",
    "    elif text == \"Nursing\":\n",
    "        major_skills = Nursing\n",
    "    elif text == \"Psychology\":\n",
    "        major_skills = Psychology\n",
    "    elif text == \"SWAP\":\n",
    "        major_skills = SWAP\n",
    "    return major_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c75b292c-2318-4824-a954-7b137c848b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soft_skills():\n",
    "    return soft_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e0dc4fc-e4e7-4f6f-a16b-6c212817e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_required_skills(text):\n",
    "    required_skills = text.split(\", \")\n",
    "    return required_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ccd6221f-781d-45fd-939c-8ae526e867b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(text, req, soft, rel):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(text)\n",
    " \n",
    "    # remove the stop words\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    " \n",
    "    # remove the punctuation\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    " \n",
    "    # generate bigrams and trigrams (such as artificial intelligence)\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    " \n",
    "    # we create a set to keep the results in.\n",
    "    found_skills = set()\n",
    " \n",
    "    # we search for each token in our skills database\n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in skills:\n",
    "            found_skills.add(token)\n",
    " \n",
    "    # we search for each bigram and trigram in our skills database\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in skills:\n",
    "            found_skills.add(ngram)\n",
    "    required_skills_included = []\n",
    "    soft_skills_included = []\n",
    "    relevant_skills_included = []\n",
    "    for skill in skills:\n",
    "        for requirement in req:\n",
    "            if skill.lower() == req.lower():\n",
    "                required_skills_included.append(skill)\n",
    "        for soft_skill in soft:\n",
    "            if skill.lower() == soft_skill.lower():\n",
    "                soft_skills_included.append(skill)\n",
    "        for relevant_skill in rel:\n",
    "            if skill.lower()== relevant_skill.lower():\n",
    "                relevant_skills_included.append(skill)\n",
    "    skills_score = ((len(required_skills_included)/len(req)*10)+(len(relevant_skills_included))+(len(soft_skills_included)))*20/30\n",
    "    return skills_score, relevant_skills_included, soft_skills_included, required_skills_included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d6d103-905c-4f02-89f7-f41bdb96b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            reader = pdf.PdfReader(f)\n",
    "            text = \"\"\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                text += reader.pages[page_num].extract_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading text from PDF: {e}\")\n",
    "        st.write(\"Error\")\n",
    "        return None\n",
    "\n",
    "def remove_square_brackets(my_list):\n",
    "    # Convert the list elements to strings\n",
    "    list_as_string = ', '.join(map(str, my_list))\n",
    "    # Remove the square brackets\n",
    "    result = list_as_string[1:-1]  # Slice the string to remove the brackets\n",
    "    return result\n",
    "\n",
    "def convert_docx_to_pdf(docx_path):\n",
    "    try:\n",
    "        # Convert DOCX to PDF\n",
    "        pdf_path = tempfile.mktemp(suffix='.pdf')\n",
    "        convert(docx_path, pdf_path)\n",
    "        return pdf_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting DOCX to PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_email(text):\n",
    "    '''\n",
    "    Helper function to extract email id from text\n",
    "\n",
    "    :param text: plain text extracted from resume file\n",
    "    '''\n",
    "    email = re.findall(r\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", text)\n",
    "    if email:\n",
    "        try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "        except IndexError:\n",
    "            return None\n",
    "\n",
    "def extract_website(text):\n",
    "    website =  re.findall(r'[a-zA-Z0-9\\.\\-+_]\\.\\+com[a-zA-Z0-9\\.\\-+_]', text)\n",
    "    if website:\n",
    "        try:\n",
    "            return website\n",
    "        except IndexError:\n",
    "            return None\n",
    "\n",
    "def remove_newlines(text):\n",
    "  \"\"\"Removes all newline characters (`\\n`) from a string.\"\"\"\n",
    "  return text.replace('\\n', '')\n",
    "\n",
    "def extract_phone_numbers(text):\n",
    "    '''\n",
    "    Function to extract phone numbers from text\n",
    "\n",
    "    :param text: plain text containing phone numbers\n",
    "    :return: list of phone numbers found in the text\n",
    "    '''\n",
    "    pattern = re.compile(r'(?<!\\n)(\\+?\\d{0,3}\\s?[-\\.\\(\\)]?\\s?\\(?\\d{3}\\)?\\s?[-\\.\\(\\)]?\\s?\\d{3}\\s?[-\\.\\(\\)]?\\s?\\d{4})')\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = re.findall(pattern, text)\n",
    "    clean_matches = [remove_newlines(match).lstrip() for match in matches]\n",
    "    # Return the list of phone numbers found\n",
    "    return clean_matches\n",
    "\n",
    "\n",
    "def extract_name_impro(full_name):\n",
    "    # Regular expression patterns for different name formats\n",
    "    patterns = [\n",
    "        # First name, middle initial, last name\n",
    "        r'^([A-Z][a-z]+)\\s+([A-Z])\\.\\s+([A-Z][a-z]+)$',\n",
    "        # First name, last name\n",
    "        r'^([A-Z][a-z]+)\\s+([A-Z][a-z]+)$',\n",
    "        # First name\n",
    "        r'^([A-Z][a-z]+)$'\n",
    "    ]\n",
    "\n",
    "    # Iterate through each pattern and attempt to match\n",
    "    for pattern in patterns:\n",
    "        match = re.match(pattern, full_name)\n",
    "        if match:\n",
    "            # Extract the parts based on the matched pattern\n",
    "            if len(match.groups()) == 3:\n",
    "                return match.group(1), match.group(2), match.group(3)\n",
    "            elif len(match.groups()) == 2:\n",
    "                return match.group(1), None, match.group(2)\n",
    "            else:\n",
    "                return match.group(1), None, None\n",
    "\n",
    "    # If no match found, return None\n",
    "    return None, None, None\n",
    "\n",
    "def extract_name(nlp_text):\n",
    "    '''\n",
    "    Helper function to extract name from spacy nlp text\n",
    "\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :param matcher: object of `spacy.matcher.Matcher`\n",
    "    :return: string of full name\n",
    "    '''\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    for pattern in cs.NAME_PATTERN:\n",
    "        matcher.add('NAME', [pattern])\n",
    "    matches = matcher(nlp_text)\n",
    "\n",
    "    for _, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        if 'name' not in span.text.lower():\n",
    "            return span.text\n",
    "\n",
    "\n",
    "\n",
    "def detect_date_format(date_str):\n",
    "    '''\n",
    "    Detects the format of the input date string.\n",
    "\n",
    "    :param date_str: Input date string\n",
    "    :return: Detected date format\n",
    "    '''\n",
    "    formats = [\n",
    "        ('%b %Y', 'Month YYYY'),\n",
    "        ('%Y-%m-%d', 'YYYY-MM-DD'),  # ISO 8601 format\n",
    "        ('%m/%d/%Y', 'MM/DD/YYYY'),  # US format\n",
    "        ('%d-%m-%Y', 'DD-MM-YYYY'),  # European/African format\n",
    "        ('%B %dst, %Y', 'Month DDst, YYYY'),  # Full month name with ordinal day\n",
    "        ('%B %dnd, %Y', 'Month DDnd, YYYY'),  # Full month name with ordinal day\n",
    "        ('%B %drd, %Y', 'Month DDrd, YYYY'),  # Full month name with ordinal day\n",
    "        ('%B %dth, %Y', 'Month DDth, YYYY')  # Full month name with ordinal day\n",
    "    ]\n",
    "\n",
    "    for date_format, format_name in formats:\n",
    "        try:\n",
    "            datetime.strptime(date_str, date_format)\n",
    "            return format_name\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return 'Unknown'\n",
    "\n",
    "\n",
    "def get_number_of_months_from_dates(date1, date2):\n",
    "    '''\n",
    "    Helper function to extract total months of experience from a resume\n",
    "\n",
    "    :param date1: Starting date\n",
    "    :param date2: Ending date\n",
    "    :return: months of experience from date1 to date2\n",
    "    '''\n",
    "    months_of_experience = 0  # Default value\n",
    "\n",
    "    if detect_date_format(date1) == 'YYYY-MM-DD':\n",
    "        if date2.lower() == 'present':\n",
    "            date2 = datetime.now().strftime('%Y-%m-%d')\n",
    "        try:\n",
    "            start_date = datetime.strptime(str(date1), '%Y-%m-%d')\n",
    "            end_date = datetime.strptime(str(date2), '%Y-%m-%d')\n",
    "            months_of_experience = (end_date.year - start_date.year) * 12 + (\n",
    "                    end_date.month - start_date.month)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    elif detect_date_format(date1) == 'MM/DD/YYYY':\n",
    "        if date2.lower() == 'present':\n",
    "            date2 = datetime.now().strftime('%m/%d/%Y')\n",
    "        try:\n",
    "            start_date = datetime.strptime(str(date1), '%m/%d/%Y')\n",
    "            end_date = datetime.strptime(str(date2), '%m/%d/%Y')\n",
    "            months_of_experience = (end_date.year - start_date.year) * 12 + (\n",
    "                    end_date.month - start_date.month)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    elif detect_date_format(date1) == 'DD-MM-YYYY':\n",
    "        if date2.lower() == 'present':\n",
    "            date2 = datetime.now().strftime('%d-%m-%Y')\n",
    "        try:\n",
    "            start_date = datetime.strptime(str(date1), '%d-%m-%Y')\n",
    "            end_date = datetime.strptime(str(date2), '%d-%m-%Y')\n",
    "            months_of_experience = (end_date.year - start_date.year) * 12 + (\n",
    "                    end_date.month - start_date.month)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    elif detect_date_format(date1) == 'Month DDst, YYYY':\n",
    "        if date2.lower() == 'present':\n",
    "            date2 = datetime.now().strftime('%B %dst, %Y')\n",
    "        try:\n",
    "            start_date = datetime.strptime(str(date1), '%B %dst, %Y')\n",
    "            end_date = datetime.strptime(str(date2), '%B %dst, %Y')\n",
    "            months_of_experience = (end_date.year - start_date.year) * 12 + (\n",
    "                    end_date.month - start_date.month)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    elif detect_date_format(date1) == 'Month DDnd, YYYY':\n",
    "        if date2.lower() == 'present':\n",
    "            date2 = datetime.now().strftime('%B %dnd, %Y')\n",
    "        try:\n",
    "            start_date = datetime.strptime(str(date1), '%B %dnd, %Y')\n",
    "            end_date = datetime.strptime(str(date2), '%B %dnd, %Y')\n",
    "            months_of_experience = (end_date.year - start_date.year) * 12 + (\n",
    "                    end_date.month - start_date.month)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    elif detect_date_format(date1) == 'Month DDrd, YYYY':\n",
    "        if date2.lower() == 'present':\n",
    "            date2 = datetime.now().strftime('%B %drd, %Y')\n",
    "        try:\n",
    "            start_date = datetime.strptime(str(date1), '%B %drd, %Y')\n",
    "            end_date = datetime.strptime(str(date2), '%B %drd, %Y')\n",
    "            months_of_experience = (end_date.year - start_date.year) * 12 + (\n",
    "                    end_date.month - start_date.month)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    elif detect_date_format(date1) == 'Month DDth, YYYY':\n",
    "        if date2.lower() == 'present':\n",
    "            date2 = datetime.now().strftime('%B %dth, %Y')\n",
    "        try:\n",
    "            start_date = datetime.strptime(str(date1), '%B %dth, %Y')\n",
    "            end_date = datetime.strptime(str(date2), '%B %dth, %Y')\n",
    "            months_of_experience = (end_date.year - start_date.year) * 12 + (\n",
    "                    end_date.month - start_date.month)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    elif detect_date_format(date1) == 'Month YYYY':\n",
    "\n",
    "        if date2.lower() == 'present':\n",
    "            date2 = datetime.now().strftime('%b %Y')\n",
    "        try:\n",
    "            if len(date1.split()[0]) > 3:\n",
    "                date1 = date1.split()\n",
    "                date1 = date1[0][:3] + ' ' + date1[1]\n",
    "            if len(date2.split()[0]) > 3:\n",
    "                date2 = date2.split()\n",
    "                date2 = date2[0][:3] + ' ' + date2[1]\n",
    "        except IndexError:\n",
    "            return 0\n",
    "        try:\n",
    "            date1 = datetime.strptime(str(date1), '%b %Y')\n",
    "            date2 = datetime.strptime(str(date2), '%b %Y')\n",
    "            months_of_experience = relativedelta(date2, date1)\n",
    "            months_of_experience = (months_of_experience.years * 12 +\n",
    "                                    months_of_experience.months)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    else:\n",
    "        try:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            return \"Can't calculate\"\n",
    "\n",
    "    return months_of_experience\n",
    "\n",
    "\n",
    "def extract_skills(nlp_text, skills_file=None):\n",
    "    '''\n",
    "    Helper function to extract skills from spacy nlp text\n",
    "\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :param noun_chunks: noun chunks extracted from nlp text\n",
    "    :return: list of skills extracted\n",
    "    '''\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    noun_chunks = nlp_text.noun_chunks\n",
    "    if not skills_file:\n",
    "        data = pd.read_csv(\n",
    "            os.path.join(os.getcwd(), 'skills.csv')\n",
    "        )\n",
    "    else:\n",
    "        data = pd.read_csv(skills_file)\n",
    "    skills = list(data.columns.values)\n",
    "    skillset = []\n",
    "    # check for one-grams\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "\n",
    "    # check for bi-grams and tri-grams\n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "\n",
    "def extract_education(nlp_text):\n",
    "    '''\n",
    "    Helper function to extract education from spacy nlp text\n",
    "\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :return: tuple of education degree and year if year if found\n",
    "             else only returns education degree\n",
    "    '''\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    try:\n",
    "        for token in nlp_text:\n",
    "            token_text = token.text.strip()\n",
    "            token_text = re.sub(r'[?|$|.|!|,]', r'', token_text)\n",
    "            if token_text.upper() in cs.EDUCATION and token_text not in cs.STOPWORDS:\n",
    "                # Concatenate current token with next token if available\n",
    "                next_token = nlp_text[token.i + 1].text if token.i + 1 < len(nlp_text) else ''\n",
    "                edu[token_text] = token.text + next_token\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(cs.YEAR), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year.group(0))))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education\n",
    "\n",
    "def extract_education_from_resume(text):\n",
    "    education = []\n",
    "\n",
    "    # Use regex pattern to find education information\n",
    "    pattern = r\"(?i)(?:Bsc|\\bB\\.\\w+|\\bM\\.\\w+|\\bPh\\.D\\.\\w+|\\bBachelor(?:'s)?|\\bMaster(?:'s)?|\\bPh\\.D)\\s(?:\\w+\\s)*\\w+\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    for match in matches:\n",
    "        education.append(match.strip())\n",
    "\n",
    "    return education\n",
    "\n",
    "def extract_entity_sections(text):\n",
    "    '''\n",
    "    Helper function to extract all the raw text from sections of\n",
    "    resume specifically for graduates and undergraduates\n",
    "\n",
    "    :param text: Raw text of resume\n",
    "    :return: dictionary of entities\n",
    "    '''\n",
    "    text_split = [i.strip() for i in text.split('\\n')]\n",
    "    entities = {}\n",
    "    key = False\n",
    "    for phrase in text_split:\n",
    "        if len(phrase) == 1:\n",
    "            p_key = phrase\n",
    "        else:\n",
    "            p_key = set(phrase.lower().split()) & set(cs.RESUME_SECTIONS)\n",
    "        try:\n",
    "            p_key = list(p_key)[0]\n",
    "        except IndexError:\n",
    "            pass\n",
    "        if p_key in cs.RESUME_SECTIONS:\n",
    "            entities[p_key] = []\n",
    "            key = p_key\n",
    "        elif key and phrase.strip():\n",
    "            entities[key].append(phrase)\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e5e5875-7272-4a07-9c20-478e4a75a189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Hello World\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f59c96-6f17-4729-886f-3abfb85844ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
